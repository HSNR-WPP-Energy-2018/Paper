%!TEX root = ../paper.tex

\section{Ausblick}

Die Funktion und Validität der Algorithmen konnte mit der geringen Anzahl gegebener Testdatensätze nicht ausreichend genug getestet und sichergestellt werden.
Mit einer größeren Menge an verfügbaren Testdaten können insbesondere die Korrelationsmatrix und der Datenbankansatz bessere Ergebnisse liefern, da Einflüsse von Ausreißern in den verfügbaren Daten ausgeglichen und eine repräsentativere Datenbasis gebildet werden können. Im Rahmen eines vollständigen Testprozesses müssen ebenfalls Sonderfälle abgedeckt und das Verhalten der Algorithmen und Heuristiken überprüft werden.

Im Rahmen der Evaluation wurden für die Konfigurationsparameter Werte verwendet, die nicht auf Basis von Fachwissen bestimmt wurden. Der Einfluss von tieferem und besseren Wissen durch Fachleute kann hierbei bereits die Ergebnisse verbessern oder zumindest realistischere Ergebnisse liefern.
Bestimmte Parameter, beispielsweise die Zusammenstellung der einfließenden Werte für die Mittelwertbildung, können möglicherweise ebenfalls gut durch einen Lernprozess bestimmt werden. Für die Mittelwertbildung ist hierzu ein Ansatz bereits implementiert, bei dem für unterschiedliche Abstände verschiedene Gewichtungen und Fensterbreiten getestet werden. Vergleichbar mit dem Testprozess wird der euklidische Abstand zwischen vollständigen Originaldaten und interpolierten Werten berechnet. Die Konfiguration, die den besten Abstandswert erzeugt, wird als Ergebnis gewählt, mit der im Weiteren interpoliert wird. Problematisch hierbei ist der benötige Rechenaufwand, der aus der Vielzahl an möglichen Kombinationen der Parameter resultiert. Möglicherweise kann der Lernprozess mit speziellen Algorithmen, bspw. ein Greedy-Algorithmus, beschleunigt werden. Es muss nicht zwingend die beste Konfiguration gefunden werden, eine gute Konfiguration reicht in der Regel ebenfalls aus. Lernverfahren für andere Algorithmen sind ebenso denkbar.

Die Algorithmen geben lediglich einfache Interpolationswerte aus. Zur Verbesserung der Interpretationsfähigkeit ist eine Ausgabe von Wahrscheinlichkeiten für die Werte hilfreich. Eine Erweiterung mit der Ausgabe verschiedener Werte mit unterschiedlicher Wahrscheinlichkeit ist ebenfalls denkbar, um verschiedene interpolierte Lastläufe anbieten zu können.

Neuronale Netze und andere komplexere Algorithmen aus dem Themengebiet des Pattern Recognition sind ebenfalls denkbare Algorithmen. Alle im Rahmen des Projektes eingesetzten Algorithmen verarbeiten alle Interpolationen auf die gleiche Art. Komplexere Ansätze können so umgesetzt werden, dass sie unterschiedliche Bereiche auf verschiedene Art und Weise interpolieren. Entscheidungsgrundlage hierbei können beispielsweise die Lückengröße oder Werteschwankungen innerhalb eines Analysefensters sein.
Anstatt einzelne, stark spezialisierte Algorithmen zu entwickeln, ist die Kombination verschiedener Algorithmen denkbar. Als Entscheidungsgrundlage können ebenfalls die genannten Kriterien verwendet werden.